import numpy as np
import matplotlib.pyplot as plt

# Generate sample data points {(x_i, y_i)}_{i=1}^n
np.random.seed(42)
n = 6  # number of data points

# Create some sample x values and y values
x_values = np.array([1, 2, 3, 4, 5, 6])
true_beta0, true_beta1 = 2.0, 1.5
y_values = true_beta0 + true_beta1 * x_values + np.random.normal(0, 0.5, n)

print("=" * 60)
print("LINEAR REGRESSION: GRADIENT AND HESSIAN OF THE LOSS")
print("=" * 60)

# Display the data
print(f"\nğŸ“Š Data: {{(x_i, y_i)}}_{{{1}}}^{{{n}}}")
for i in range(n):
    print(f"(x_{i+1}, y_{i+1}) = ({x_values[i]:.1f}, {y_values[i]:.2f})")

# Parameters Î² (starting with some initial guess)
beta0_hat = 1.0  # intercept estimate
beta1_hat = 1.0  # slope estimate
beta = np.array([[beta0_hat], [beta1_hat]])

print(f"\nğŸ”¢ Initial Parameters: Î² = [Î²â‚€, Î²â‚]áµ€ = [{beta0_hat}, {beta1_hat}]áµ€")

# Design matrix
X = np.column_stack([np.ones(n), x_values])
y = y_values.reshape(-1, 1)

print(f"\nğŸ“ Design Matrix X and response vector y:")
print("X =")
print(X)
print(f"y = {y.flatten()}")

# Calculate residuals
residuals = X @ beta - y  # r = XÎ² - y
print(f"\nğŸ“ Residuals: r = XÎ² - y")
print(f"r = {residuals.flatten()}")

# Calculate current loss
current_loss = (1/(2*n)) * np.sum(residuals**2)
print(f"\nğŸ¯ Current Loss: L(Î²) = (1/2n)||r||â‚‚Â² = {current_loss:.4f}")

print(f"\n" + "="*60)
print("GRADIENT OF THE LOSS (First Derivative)")
print("="*60)

# Calculate gradient: âˆ‡L(Î²) = (1/n)Xáµ€(XÎ² - y) = (1/n)Xáµ€r
gradient = (1/n) * X.T @ residuals
print(f"\nğŸ” Gradient Formula: âˆ‡L(Î²) = (1/n)Xáµ€(XÎ² - y) = (1/n)Xáµ€r")

print(f"\nStep-by-step calculation:")
print(f"Xáµ€ = ")
print(X.T)
print(f"Xáµ€r = ")
print((X.T @ residuals).flatten())
print(f"âˆ‡L(Î²) = (1/{n}) Ã— {(X.T @ residuals).flatten()} = {gradient.flatten()}")

print(f"\nğŸ’¡ Interpretation:")
print(f"   â€¢ âˆ‡L/âˆ‚Î²â‚€ = {gradient[0,0]:.4f}: slope w.r.t. intercept")
print(f"   â€¢ âˆ‡L/âˆ‚Î²â‚ = {gradient[1,0]:.4f}: slope w.r.t. slope parameter")
print(f"   â€¢ Gradient points toward steepest ascent")
print(f"   â€¢ Negative gradient points toward steepest descent")

print(f"\n" + "="*60)
print("HESSIAN OF THE LOSS (Second Derivative)")
print("="*60)

# Calculate Hessian: âˆ‡Â²L(Î²) = (1/n)Xáµ€X
hessian = (1/n) * X.T @ X
print(f"\nğŸ” Hessian Formula: âˆ‡Â²L(Î²) = (1/n)Xáµ€X")

print(f"\nStep-by-step calculation:")
print(f"Xáµ€X = ")
print(X.T @ X)
print(f"âˆ‡Â²L(Î²) = (1/{n}) Ã— Xáµ€X = ")
print(hessian)

# Analyze Hessian properties
eigenvalues = np.linalg.eigvals(hessian)
condition_number = np.linalg.cond(hessian)
print(f"\nğŸ“Š Hessian Properties:")
print(f"   â€¢ Eigenvalues: {eigenvalues}")
print(f"   â€¢ Condition number: {condition_number:.2f}")
print(f"   â€¢ Positive definite: {np.all(eigenvalues > 0)}")
print(f"   â€¢ Independent of Î²: curvature same everywhere")

print(f"\n" + "="*60)
print("GRADIENT DESCENT UPDATE")
print("="*60)

# Gradient descent update: Î²_{k+1} = Î²_k - Î·âˆ‡L(Î²_k)
learning_rate = 0.2
beta_new = beta - learning_rate * gradient

print(f"\nğŸš€ Update Rule: Î²_{{k+1}} = Î²_k - Î·âˆ‡L(Î²_k)")
print(f"Learning rate Î· = {learning_rate}")
print(f"\nUpdate step:")
print(f"Î²_{{k+1}} = {beta.flatten()} - {learning_rate} Ã— {gradient.flatten()}")
print(f"Î²_{{k+1}} = {beta.flatten()} - {(learning_rate * gradient).flatten()}")
print(f"Î²_{{k+1}} = {beta_new.flatten()}")

# Calculate new loss
residuals_new = X @ beta_new - y
new_loss = (1/(2*n)) * np.sum(residuals_new**2)
print(f"\nğŸ“‰ Loss improvement: {current_loss:.4f} â†’ {new_loss:.4f}")
print(f"   Loss reduction: {current_loss - new_loss:.4f}")

print(f"\n" + "="*60)
print("NEWTON'S METHOD UPDATE")
print("="*60)

# Newton's method: Î²_{k+1} = Î²_k - (âˆ‡Â²L)^{-1}âˆ‡L
hessian_inv = np.linalg.inv(hessian)
beta_newton = beta - hessian_inv @ gradient

print(f"\nğŸ¯ Newton's Method: Î²_{{k+1}} = Î²_k - (âˆ‡Â²L)^{{-1}}âˆ‡L")
print(f"\nStep-by-step:")
print(f"(âˆ‡Â²L)^{{-1}} = ")
print(hessian_inv)
print(f"(âˆ‡Â²L)^{{-1}}âˆ‡L = ")
print((hessian_inv @ gradient).flatten())
print(f"Î²_{{k+1}} = {beta.flatten()} - {(hessian_inv @ gradient).flatten()}")
print(f"Î²_{{k+1}} = {beta_newton.flatten()}")

# This should be the analytical solution
residuals_newton = X @ beta_newton - y
newton_loss = (1/(2*n)) * np.sum(residuals_newton**2)
print(f"\nğŸ“‰ Newton's method loss: {newton_loss:.6f}")

# Verify this is the analytical solution
beta_analytical = np.linalg.inv(X.T @ X) @ (X.T @ y)
print(f"âœ“ Analytical solution: Î²Ì‚ = (Xáµ€X)^{{-1}}Xáµ€y = {beta_analytical.flatten()}")

print(f"\n" + "="*60)
print("GEOMETRIC INTERPRETATION")
print("="*60)

print(f"\nğŸ” 1D Quadratic Analogy:")
print(f"   For L(Î¸) = aÎ¸Â² + bÎ¸ + c:")
print(f"   â€¢ Gradient: dL/dÎ¸ = 2aÎ¸ + b (slope)")
print(f"   â€¢ Hessian: dÂ²L/dÎ¸Â² = 2a (curvature)")

print(f"\nğŸ” Multivariate Case (Our Problem):")
print(f"   â€¢ L(Î²) = (1/2n)||XÎ² - y||â‚‚Â²")
print(f"   â€¢ âˆ‡L(Î²) = (1/n)Xáµ€(XÎ² - y) (slope vector)")
print(f"   â€¢ âˆ‡Â²L(Î²) = (1/n)Xáµ€X (curvature matrix)")

print(f"\nğŸ’¡ Key Insights:")
print(f"   â€¢ Gradient: direction of steepest ascent")
print(f"   â€¢ -Gradient: direction of steepest descent") 
print(f"   â€¢ Hessian: reshapes descent directions")
print(f"   â€¢ Condition number affects convergence speed")

# Visualization
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

# Plot 1: Data and current fit
x_line = np.linspace(0, 7, 100)
y_line_current = beta[0,0] + beta[1,0] * x_line
y_line_optimal = beta_analytical[0,0] + beta_analytical[1,0] * x_line

ax1.scatter(x_values, y_values, color='blue', s=100, alpha=0.7, label='Data')
ax1.plot(x_line, y_line_current, 'r--', linewidth=2, label=f'Current: L={current_loss:.4f}')
ax1.plot(x_line, y_line_optimal, 'g-', linewidth=2, label=f'Optimal: L={newton_loss:.6f}')
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.set_title('Current vs Optimal Fit')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: Gradient vector field
beta0_range = np.linspace(-1, 4, 10)
beta1_range = np.linspace(0, 3, 10)
B0, B1 = np.meshgrid(beta0_range, beta1_range)
Grad0 = np.zeros_like(B0)
Grad1 = np.zeros_like(B1)

for i in range(len(beta0_range)):
    for j in range(len(beta1_range)):
        beta_test = np.array([[B0[j,i]], [B1[j,i]]])
        grad_test = (1/n) * X.T @ (X @ beta_test - y)
        Grad0[j,i] = grad_test[0,0]
        Grad1[j,i] = grad_test[1,0]

ax2.quiver(B0, B1, -Grad0, -Grad1, alpha=0.6)  # Negative for descent direction
ax2.plot(beta[0,0], beta[1,0], 'ro', markersize=10, label='Current Î²')
ax2.plot(beta_analytical[0,0], beta_analytical[1,0], 'g*', markersize=15, label='Optimal Î²')
ax2.set_xlabel('Î²â‚€')
ax2.set_ylabel('Î²â‚')
ax2.set_title('Gradient Vector Field (Descent Directions)')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Plot 3: Loss contours
Loss_surface = np.zeros_like(B0)
for i in range(len(beta0_range)):
    for j in range(len(beta1_range)):
        beta_test = np.array([[B0[j,i]], [B1[j,i]]])
        residuals_test = X @ beta_test - y
        Loss_surface[j,i] = (1/(2*n)) * np.sum(residuals_test**2)

contour = ax3.contour(B0, B1, Loss_surface, levels=15)
ax3.clabel(contour, inline=True, fontsize=8)
ax3.plot(beta[0,0], beta[1,0], 'ro', markersize=10, label='Current Î²')
ax3.plot(beta_analytical[0,0], beta_analytical[1,0], 'g*', markersize=15, label='Optimal Î²')
ax3.set_xlabel('Î²â‚€')
ax3.set_ylabel('Î²â‚')
ax3.set_title('Loss Function L(Î²â‚€,Î²â‚)')
ax3.legend()
ax3.grid(True, alpha=0.3)

# Plot 4: Hessian eigenvalue visualization
theta = np.linspace(0, 2*np.pi, 100)
# Create ellipse based on Hessian eigenvalues and eigenvectors
eigenvals, eigenvecs = np.linalg.eig(hessian)
angle = np.arctan2(eigenvecs[1,0], eigenvecs[0,0])

# Ellipse parameters
a = 1/np.sqrt(eigenvals[0])  # Semi-major axis
b = 1/np.sqrt(eigenvals[1])  # Semi-minor axis

# Parametric ellipse
ellipse_x = a * np.cos(theta)
ellipse_y = b * np.sin(theta)

# Rotate ellipse
cos_angle = np.cos(angle)
sin_angle = np.sin(angle)
x_rotated = ellipse_x * cos_angle - ellipse_y * sin_angle + beta_analytical[0,0]
y_rotated = ellipse_x * sin_angle + ellipse_y * cos_angle + beta_analytical[1,0]

ax4.plot(x_rotated, y_rotated, 'b-', linewidth=2, label='Hessian Ellipse')
ax4.plot(beta_analytical[0,0], beta_analytical[1,0], 'g*', markersize=15, label='Optimal Î²')
ax4.arrow(beta_analytical[0,0], beta_analytical[1,0], 
          eigenvecs[0,0]*a, eigenvecs[1,0]*a, 
          head_width=0.05, head_length=0.05, fc='red', ec='red')
ax4.arrow(beta_analytical[0,0], beta_analytical[1,0], 
          eigenvecs[0,1]*b, eigenvecs[1,1]*b, 
          head_width=0.05, head_length=0.05, fc='orange', ec='orange')
ax4.set_xlabel('Î²â‚€')
ax4.set_ylabel('Î²â‚')
ax4.set_title('Hessian Curvature (Eigenvalue Ellipse)')
ax4.legend()
ax4.grid(True, alpha=0.3)
ax4.axis('equal')

plt.tight_layout()
plt.show()

print(f"\nğŸ¯ SUMMARY:")
print(f"   â€¢ Gradient âˆ‡L(Î²) = (1/n)Xáµ€r gives descent direction")
print(f"   â€¢ Hessian âˆ‡Â²L(Î²) = (1/n)Xáµ€X gives curvature information")
print(f"   â€¢ Gradient descent: Î²_{{k+1}} = Î²_k - Î·âˆ‡L(Î²_k)")
print(f"   â€¢ Newton's method: Î²_{{k+1}} = Î²_k - (âˆ‡Â²L)^{{-1}}âˆ‡L(Î²_k)")
print(f"   â€¢ Convex quadratic bowl â†’ unique global minimum")
import numpy as np
import matplotlib.pyplot as plt

# Generate sample data points {(x_i, y_i)}_{i=1}^n
np.random.seed(42)
n = 6  # number of data points

# Create some sample x values
x_values = np.array([1, 2, 3, 4, 5, 6])

# Create y values with some linear relationship plus noise
# True relationship: y = 2 + 1.5*x + noise
true_beta0, true_beta1 = 2.0, 1.5
y_values = true_beta0 + true_beta1 * x_values + np.random.normal(0, 0.5, n)

print("=" * 60)
print("LINEAR REGRESSION SETUP & NOTATION")
print("=" * 60)

# Display the data
print(f"\nğŸ“Š Data: {{(x_i, y_i)}}_{{{1}}}^{{{n}}}")
print("-" * 30)
for i in range(n):
    print(f"(x_{i+1}, y_{i+1}) = ({x_values[i]:.1f}, {y_values[i]:.2f})")

# Parameters Î² = [Î²â‚€, Î²â‚]áµ€ (we'll estimate these)
# For demonstration, let's use some estimated values
beta0_hat = 1.8  # intercept estimate
beta1_hat = 1.6  # slope estimate
beta = np.array([[beta0_hat], [beta1_hat]])

print(f"\nğŸ”¢ Parameters: Î² = [Î²â‚€, Î²â‚]áµ€")
print("-" * 30)
print(f"Î²â‚€ (intercept) = {beta0_hat}")
print(f"Î²â‚ (slope) = {beta1_hat}")
print(f"Î² = {beta.flatten()}")

# Design matrix X and vectors
print(f"\nğŸ“ Design Matrix X âˆˆ â„^{{{n}Ã—2}} and Vectors")
print("-" * 40)

# Design matrix: X = [ğŸ™ | x] where ğŸ™ is column of ones
ones_column = np.ones(n)
X = np.column_stack([ones_column, x_values])
print("X = [ğŸ™ | x] =")
print(X)

# Response vector y
y = y_values.reshape(-1, 1)  # column vector
print(f"\ny = (yâ‚, ..., y_{n})áµ€ =")
print(y.flatten())

# Predicted values: Å· = XÎ²
y_hat = X @ beta
print(f"\nÅ· = XÎ² =")
print(y_hat.flatten())

# Show the matrix multiplication step by step
print(f"\nğŸ” Matrix Multiplication Breakdown:")
print(f"Å·áµ¢ = Î²â‚€ + Î²â‚xáµ¢")
for i in range(n):
    pred_i = beta0_hat + beta1_hat * x_values[i]
    print(f"Å·_{i+1} = {beta0_hat} + {beta1_hat}Ã—{x_values[i]} = {pred_i:.2f}")

# Residuals (errors): r = Å· - y (note: this is predicted minus actual)
print(f"\nğŸ“ Residuals (errors): ráµ¢ = Å·áµ¢ - yáµ¢")
print("-" * 40)
residuals = y_hat - y
print("Individual residuals:")
for i in range(n):
    print(f"r_{i+1} = Å·_{i+1} - y_{i+1} = {y_hat[i,0]:.2f} - {y_values[i]:.2f} = {residuals[i,0]:.2f}")

print(f"\nr = XÎ² - y =")
print(residuals.flatten())

# Alternative formula shown in the notation: ráµ¢ = Î²â‚€ + Î²â‚xáµ¢ - yáµ¢
print(f"\nğŸ”„ Alternative calculation: ráµ¢ = Î²â‚€ + Î²â‚xáµ¢ - yáµ¢")
for i in range(n):
    alt_residual = beta0_hat + beta1_hat * x_values[i] - y_values[i]
    print(f"r_{i+1} = {beta0_hat} + {beta1_hat}Ã—{x_values[i]} - {y_values[i]:.2f} = {alt_residual:.2f}")

# Summary statistics
print(f"\nğŸ“Š Summary Statistics")
print("-" * 25)
print(f"Sum of residuals: {np.sum(residuals):.3f}")
print(f"Sum of squared residuals: {np.sum(residuals**2):.3f}")
print(f"Mean squared error: {np.mean(residuals**2):.3f}")

# Visualization
plt.figure(figsize=(10, 6))

# Plot data points
plt.scatter(x_values, y_values, color='blue', s=100, alpha=0.7, label='Data points (xáµ¢, yáµ¢)')

# Plot regression line
x_line = np.linspace(0, 7, 100)
y_line = beta0_hat + beta1_hat * x_line
plt.plot(x_line, y_line, 'r-', linewidth=2, label=f'Å· = {beta0_hat} + {beta1_hat}x')

# Plot predicted points
plt.scatter(x_values, y_hat.flatten(), color='red', s=80, alpha=0.7, marker='x', label='Predicted Å·áµ¢')

# Draw residual lines
for i in range(n):
    plt.plot([x_values[i], x_values[i]], [y_values[i], y_hat[i,0]], 'g--', alpha=0.7)

plt.xlabel('x')
plt.ylabel('y')
plt.title('Linear Regression: Data, Predictions, and Residuals')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nâœ… The green dashed lines show the residuals (errors)")
print(f"   Each residual ráµ¢ represents the vertical distance from yáµ¢ to Å·áµ¢")